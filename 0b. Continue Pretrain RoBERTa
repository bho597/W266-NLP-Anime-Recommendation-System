{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"dC9SAnthdHXb","executionInfo":{"status":"ok","timestamp":1713084536325,"user_tz":420,"elapsed":2,"user":{"displayName":"Brendan Ho","userId":"08949042960870142991"}}},"outputs":[],"source":["# brendanho123\n","# PATH = 'drive/My Drive/Colab Notebooks/W266/Final Project/'\n","\n","# bholly597\n","PATH = 'drive/My Drive/Final Project/'"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# standard libraries\n","import pandas as pd\n","import numpy as np\n","import math\n","\n","# train/val/test split\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","\n","# for models\n","import tensorflow as tf\n","\n","!pip install -q transformers==4.37.2\n","import transformers\n","from transformers import pipeline, TFRobertaModel, TFRobertaForMaskedLM, RobertaTokenizer, DataCollatorForLanguageModeling\n","\n","!pip install datasets -q\n","import datasets\n","\n","# for visualization\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# for evaluation metrics\n","from sklearn.metrics import classification_report\n","\n","!pip install evaluate -q\n","from evaluate import load"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4lyqyYtEdI6R","outputId":"451fadde-1063-4b1a-e6a3-f43b68cd8e46","executionInfo":{"status":"ok","timestamp":1713084587820,"user_tz":420,"elapsed":51496,"user":{"displayName":"Brendan Ho","userId":"08949042960870142991"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import pickle"],"metadata":{"id":"IRpymgLfecki","executionInfo":{"status":"ok","timestamp":1713084587820,"user_tz":420,"elapsed":3,"user":{"displayName":"Brendan Ho","userId":"08949042960870142991"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["CHECKPOINT = 'roberta-base'\n","TOKENIZER = RobertaTokenizer.from_pretrained(CHECKPOINT)\n","ROBERTA_MODEL = TFRobertaModel.from_pretrained(CHECKPOINT)\n","ROBERTA_MLM = TFRobertaForMaskedLM.from_pretrained(CHECKPOINT)\n","\n","# per RoBERTa default\n","MAX_LEN = 512\n","VOCAB_SIZE = 50265\n","HIDDEN_DIM = 768\n","\n","BATCH_SIZE = 8\n"],"metadata":{"id":"2Gg6fnz9dQh-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713084599439,"user_tz":420,"elapsed":11622,"user":{"displayName":"Brendan Ho","userId":"08949042960870142991"}},"outputId":"efc1bd7d-458b-46c7-f710-c015b4d1865a"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.embeddings.position_ids']\n","- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForMaskedLM: ['roberta.embeddings.position_ids']\n","- This IS expected if you are initializing TFRobertaForMaskedLM from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFRobertaForMaskedLM from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFRobertaForMaskedLM were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForMaskedLM for predictions without further training.\n"]}]},{"cell_type":"markdown","metadata":{"id":"AcEQwd6YmLgK"},"source":["# Define reusable functions"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"v3rNzaA-mNit","executionInfo":{"status":"ok","timestamp":1713084599439,"user_tz":420,"elapsed":11,"user":{"displayName":"Brendan Ho","userId":"08949042960870142991"}}},"outputs":[],"source":["def to_tokenize(input):\n","  return TOKENIZER(input,\n","                   add_special_tokens=True,\n","                   max_length=MAX_LEN,\n","                   padding='max_length',\n","                   return_token_type_ids=True,\n","                   truncation=True,\n","                   return_tensors=\"tf\"\n","                   )"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"nzQCw5KEs6c6","executionInfo":{"status":"ok","timestamp":1713084599439,"user_tz":420,"elapsed":10,"user":{"displayName":"Brendan Ho","userId":"08949042960870142991"}}},"outputs":[],"source":["def show_results(model, feature, label, classes):\n","  yhat_val = model.predict(feature)\n","  yhat_val_result = np.argmax(yhat_val, axis=-1)\n","\n","  print('Validation classification Report \\n')\n","  print(classification_report(label, yhat_val_result, target_names=classes))\n","\n","  ax = sns.heatmap(tf.math.confusion_matrix(label, yhat_val_result),\n","                 annot=True,\n","                 fmt='.0f',\n","                 cmap='Blues',\n","                 yticklabels=classes,\n","                 xticklabels=classes,\n","                 cbar=False)\n","\n","  ax.set(xlabel='Predicted Label', ylabel='True Label')\n","  plt.title('Validation Confusion Matrix')\n","  plt.show()"]},{"cell_type":"code","source":[],"metadata":{"id":"bDlzUjfxdKJx","executionInfo":{"status":"ok","timestamp":1713084599439,"user_tz":420,"elapsed":10,"user":{"displayName":"Brendan Ho","userId":"08949042960870142991"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3yll8eNa33R-"},"source":["# Load Dataset"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"5c-hesq55bo0","executionInfo":{"status":"ok","timestamp":1713084601978,"user_tz":420,"elapsed":2549,"user":{"displayName":"Brendan Ho","userId":"08949042960870142991"}}},"outputs":[],"source":["cleaned_data_path = f'{PATH}data/cleaned_data'\n","\n","\n","unpickled_data = {}\n","\n","keys_to_unpickle = [\n","    'train_data',\n","    'test_data',\n","    'val_data',\n","]\n","\n","\n","for key in keys_to_unpickle:\n","    filename = f'{cleaned_data_path}/{key}.pkl'\n","    with open(filename, 'rb') as file:\n","        unpickled_data[key] = pickle.load(file)\n","\n","\n","train_data = unpickled_data['train_data']\n","test_data = unpickled_data['test_data']\n","val_data = unpickled_data['val_data']\n"]},{"cell_type":"code","source":["num_train_examples = 2000\n","num_test_examples = 200\n","num_val_examples = 200\n","\n","X_train_synopsis = train_data['synopsis'][:num_train_examples]\n","X_test_synopsis = test_data['synopsis'][:num_test_examples]\n","X_val_synopsis = val_data['synopsis'][:num_val_examples]\n","\n","X_train_reviews = train_data['text'][:num_train_examples]\n","X_test_reviews = test_data['text'][:num_test_examples]\n","X_val_reviews = val_data['text'][:num_val_examples]\n","\n"],"metadata":{"id":"v2lD6YFpeGOJ","executionInfo":{"status":"ok","timestamp":1713084601978,"user_tz":420,"elapsed":4,"user":{"displayName":"Brendan Ho","userId":"08949042960870142991"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["X_train_raw = pd.concat([X_train_synopsis, X_train_reviews]).sample(frac=1).reset_index(drop=True).astype(str).tolist()\n","X_test_raw = pd.concat([X_test_synopsis, X_test_reviews]).sample(frac=1).reset_index(drop=True).astype(str).tolist()\n","X_val_raw = pd.concat([X_val_synopsis, X_val_reviews]).sample(frac=1).reset_index(drop=True).astype(str).tolist()\n","\n","X_train = [item.replace('[Written by MAL Rewrite]', '').strip() for item in X_train_raw]\n","X_test = [item.replace('[Written by MAL Rewrite]', '').strip() for item in X_test_raw]\n","X_val = [item.replace('[Written by MAL Rewrite]', '').strip() for item in X_val_raw]\n","\n","\n","print('--- training set ---')\n","print('number of samples: ', len(X_train))\n","\n","print('--- validation set ---')\n","print('number of samples: ', len(X_test))\n","\n","print('--- test set ---')\n","print('number of samples: ', len(X_val))"],"metadata":{"id":"EpMI8f-Me0TI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713084601978,"user_tz":420,"elapsed":3,"user":{"displayName":"Brendan Ho","userId":"08949042960870142991"}},"outputId":"c7a3dba2-27bf-40f5-a41a-97a9bb997daf"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["--- training set ---\n","number of samples:  4000\n","--- validation set ---\n","number of samples:  400\n","--- test set ---\n","number of samples:  400\n"]}]},{"cell_type":"code","source":["X_train[:4]"],"metadata":{"id":"sP9L5C4affQ4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713084601978,"user_tz":420,"elapsed":3,"user":{"displayName":"Brendan Ho","userId":"08949042960870142991"}},"outputId":"c6fad438-14f1-4585-b312-716b8ee48163"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['This is my first review so take it easy Kore wa Zombie Desu ka? Is an anime not mean to be taken seriously. Its story and characters are light hearted and easy to understand but also able to make to laugh and brighten your day. I highly recommend people to watch this if your looking for a funny but also interesting anime.',\n"," \"Sword Art Online. A series nobody ever fails to hate on. It's gotten to the point where people are hating on it whilst not even watching the anime, and not understanding how much thought was put in. And while S1 and S2 were not the best, A-1 definitely learned, resulting in the beauty that is Alicization; more specifically, War of Underworld. ** MAY CONTAIN MINOR SPOILERS** Story: 10/10 This is basically Alicization S2, so the story follows from there. The concept of Alicization is incredible in itself. Real souls put inside virtual people to create a real world that doesn't exist. And to add to that, Kirito is now being protected by one, even though he is from the real world. War of Underworld massively develops on the consequences of this. We always see how the hero saves the day, but what's the reverse of that? This shows exactly how the simple act of saving people can have the adverse affect, and is portrayed incredibly. Art: 10/10 A-1 have outdone themselves. Especially towards the end, as the battle scenes increased, the animation just got better. The fights are absolutely incredible now, and because of that, SAO contains my favourite scene from any anime in 2019. Alicization in general had a refreshed feel to it's animations. War of Underworld took that one step further, creating intense battle scenes that just became better to watch as the show continued. I really hope they keep this up. Sound: 8/10 Honestly, the only reason I didn't rate it 10/10 is because of how annoying the OP is. It kills the mood of the fight scenes in a few episodes. Apart from that, the sound has always been brilliant in SAO. This season particularly had an incredible ED, which suited so so well with the season and content. LiSA has never disappointed in an anime OP or ED. The battle music is very good and fitting too, especially considering the nature of the battles. Characters: 8/10 On the contrary, SAO has always had problems with character development. Alice definitely proved that wrong. Kirito is now in a state where he doesn't move, which honestly is refreshing too, as he no longer deserves the limelight, having been so overpowered. Alice is more balanced, and a much better character in general. Most other characters from previous seasons don't appear, so I won't explain anyone else. Enjoyment: 10/10 Every single episode had me on the edge of my seat. The execution of this season was brilliant, and I don't remember any other anime where I was waiting so eagerly each week to watch it. This season was a whole level above SAO and SAO S2, and even Alicization part 1. I'm extremely excited for S2. Overall: 10/10 Alicizarion would be rated so much higher had it not had connections with the infamous SAO series. However, Alicization is not at all for everyone. If you're after a nice, happy ending, do not watch this. If you're after an intense story, where characters are battling constantly, fights becoming more and more thrilling by the second and a genuinely disgusting villain, go ahead. But of course, watch the rest of SAO first.\",\n"," 'Thought your life was bad? Sometimes, death is worse. There is no salvation, peace, nor god waiting to receive you into their care. But wait, a god? Maybe you are talking about that big black ball stuck in the room with you. Now you are thrown into a game, fighting green aliens and robot monsters for the chance to survive.      When Kei Kurono is killed, he thus finds himself caught in such a game—a test of his skills, morals, and will to survive. His life is not his own; his death is spat and trampled upon over and over again. What happens if he does not listen? God knows.      A word of warning: Gantz is not for the faint-hearted, but neither is it as simple as it looks. Gore, rape, and violence is rampant, as are portrayals of greed, violence, and all the ugliness that one sees in society today.',\n"," 'Just as in this life, the afterlife needs a calm troubleshooter to deal with the bureaucratic headaches that come from keeping things in order. Enter Hozuki: a cool and collected demon who’s badly in need of a vacation.      (Source: HIDIVE)']"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["X_train_tokenized = to_tokenize(X_train)\n","X_val_tokenized = to_tokenize(X_val)\n","X_test_tokenized = to_tokenize(X_test)"],"metadata":{"id":"AXwXLo_de9hQ","executionInfo":{"status":"ok","timestamp":1713084627404,"user_tz":420,"elapsed":25428,"user":{"displayName":"Brendan Ho","userId":"08949042960870142991"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["X_train_repeated = [item for item in X_train for _ in range(10)]\n","X_val_repeated = [item for item in X_val for _ in range(10)]\n","\n","len(X_train_repeated)"],"metadata":{"id":"5-VESbIHfc0w","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713084627405,"user_tz":420,"elapsed":11,"user":{"displayName":"Brendan Ho","userId":"08949042960870142991"}},"outputId":"ab030042-c1c8-48d0-c61e-4401ac21db0a"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["40000"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["X_train_repeated_tokenized = to_tokenize(X_train_repeated)\n","X_train_repeated_tokenized['labels'] = X_train_repeated_tokenized['input_ids']\n","\n","X_val_repeated_tokenized = to_tokenize(X_val_repeated)\n","X_val_repeated_tokenized['labels'] = X_val_repeated_tokenized['input_ids']"],"metadata":{"id":"MeyV5zRKmV3u","executionInfo":{"status":"ok","timestamp":1713084740256,"user_tz":420,"elapsed":112860,"user":{"displayName":"Brendan Ho","userId":"08949042960870142991"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["reformat_train_sentence = datasets.Dataset.from_dict({key: X_train_repeated_tokenized[key].numpy() for key in X_train_repeated_tokenized.keys()})\n","reformat_val_sentence = datasets.Dataset.from_dict({key: X_val_repeated_tokenized[key].numpy() for key in X_val_repeated_tokenized.keys()})"],"metadata":{"id":"ZqC9_PAjmbnU","executionInfo":{"status":"ok","timestamp":1713084742922,"user_tz":420,"elapsed":2675,"user":{"displayName":"Brendan Ho","userId":"08949042960870142991"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["def continue_pretrain(mlm_model=ROBERTA_MLM,\n","                      train_data=reformat_train_sentence,\n","                      val_data=reformat_val_sentence,\n","                      continue_flag=True):\n","\n","  # Use DataCollatorForLanguageModeling to implement `dynamic masking`\n","  data_collator = DataCollatorForLanguageModeling(tokenizer=TOKENIZER,\n","                                                  mlm_probability=0.15,\n","                                                  return_tensors='tf')\n","\n","  train_dataset = mlm_model.prepare_tf_dataset(reformat_train_sentence,\n","                                               collate_fn=data_collator,\n","                                               shuffle=True,\n","                                               batch_size=BATCH_SIZE\n","                                               )\n","\n","  val_dataset = mlm_model.prepare_tf_dataset(reformat_val_sentence,\n","                                             collate_fn=data_collator,\n","                                             shuffle=True,\n","                                             batch_size=BATCH_SIZE\n","                                             )\n","\n","  LR_SCHEDULE = tf.keras.optimizers.schedules.PolynomialDecay(initial_learning_rate=1e-6,\n","                                                            decay_steps=5336,\n","                                                            end_learning_rate=1e-9,\n","                                                            power=1.0)\n","  OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=LR_SCHEDULE,\n","                                     beta_1=0.9,\n","                                     beta_2=0.98,\n","                                     epsilon=1e-06,\n","                                     clipnorm=0.0)\n","\n","  mlm_model.compile(optimizer=OPTIMIZER)\n","\n","  # fit the model if continue pretrain, otherwise return the compiled original model\n","  if not continue_flag:\n","    loss = mlm_model.evaluate(val_dataset)\n","    print(f\"Original Perplexity: {math.exp(loss):.2f}\")\n","  else:\n","    mlm_model.fit(train_dataset,\n","                  validation_data=val_dataset,\n","                  batch_size=BATCH_SIZE,\n","                  epochs=5,\n","                  verbose=1\n","                  )\n","    loss = mlm_model.evaluate(val_dataset)\n","    print(f\"Trained Perplexity: {math.exp(loss):.2f}\")\n","\n","  return mlm_model"],"metadata":{"id":"RPwoKuMRmftu","executionInfo":{"status":"ok","timestamp":1713084742923,"user_tz":420,"elapsed":3,"user":{"displayName":"Brendan Ho","userId":"08949042960870142991"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["new_pretrained = continue_pretrain(continue_flag=True)"],"metadata":{"id":"ff7bdlVbmwlV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713095173562,"user_tz":420,"elapsed":10430641,"user":{"displayName":"Brendan Ho","userId":"08949042960870142991"}},"outputId":"cba39490-648a-45a4-99ef-b2ffadbfad2f"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","5000/5000 [==============================] - 2115s 414ms/step - loss: 1.3882 - val_loss: 1.2893\n","Epoch 2/5\n","5000/5000 [==============================] - 2063s 413ms/step - loss: 1.3730 - val_loss: 1.3019\n","Epoch 3/5\n","5000/5000 [==============================] - 2051s 410ms/step - loss: 1.3716 - val_loss: 1.2954\n","Epoch 4/5\n","5000/5000 [==============================] - 2056s 411ms/step - loss: 1.3740 - val_loss: 1.2959\n","Epoch 5/5\n","5000/5000 [==============================] - 2058s 412ms/step - loss: 1.3748 - val_loss: 1.2990\n","500/500 [==============================] - 74s 148ms/step - loss: 1.2901\n","Trained Perplexity: 3.63\n"]}]},{"cell_type":"code","source":["pretrain_path = f'{PATH}data/RoBERTa_continue_pretrained'\n","new_pretrained.save_pretrained(pretrain_path)"],"metadata":{"id":"5GVqVBcp0eW5","executionInfo":{"status":"ok","timestamp":1713095178758,"user_tz":420,"elapsed":5198,"user":{"displayName":"Brendan Ho","userId":"08949042960870142991"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"x9kBYQ7HaujS"},"execution_count":null,"outputs":[]}]}